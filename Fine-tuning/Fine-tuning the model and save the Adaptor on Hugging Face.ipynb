{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine Tuning Google Gemma ( Gemma-2b-it )**","metadata":{}},{"cell_type":"markdown","source":"Import libraries :","metadata":{}},{"cell_type":"code","source":"%%capture \n%pip install -U bitsandbytes \n%pip install -U transformers \n%pip install -U peft \n%pip install -U accelerate \n%pip install -U trl\n%pip install -U datasets\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n!pip install -q tensorflow-cpu\n!pip install -q -U keras-nlp tensorflow-hub\n!pip install -q -U keras>=3\n!pip install -q -U tensorflow-text","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Update transformers and then restart the kernel :","metadata":{}},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Start the Fine Tuning :","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:48:57.542546Z","iopub.execute_input":"2024-06-04T10:48:57.543256Z","iopub.status.idle":"2024-06-04T10:49:06.571234Z","shell.execute_reply.started":"2024-06-04T10:48:57.543217Z","shell.execute_reply":"2024-06-04T10:49:06.570228Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"base_model = \"google/gemma-2b-it\"\ndataset_name = \"aymanboufarhi/fstt_data\"\nnew_model = \"chat-bot-fstt\"","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:49:09.318718Z","iopub.execute_input":"2024-06-04T10:49:09.319553Z","iopub.status.idle":"2024-06-04T10:49:09.324172Z","shell.execute_reply.started":"2024-06-04T10:49:09.319518Z","shell.execute_reply":"2024-06-04T10:49:09.323141Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:49:11.360834Z","iopub.execute_input":"2024-06-04T10:49:11.361720Z","iopub.status.idle":"2024-06-04T10:49:11.370958Z","shell.execute_reply.started":"2024-06-04T10:49:11.361687Z","shell.execute_reply":"2024-06-04T10:49:11.370121Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"key of hugging face for read","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token $'read key'","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:49:13.418317Z","iopub.execute_input":"2024-06-04T10:49:13.418698Z","iopub.status.idle":"2024-06-04T10:49:15.078251Z","shell.execute_reply.started":"2024-06-04T10:49:13.418668Z","shell.execute_reply":"2024-06-04T10:49:15.077039Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"add your wandb api token in Kaggle secrets [ click (Add-ons) ]","metadata":{}},{"cell_type":"code","source":"secret_wandb = user_secrets.get_secret(\"wandb\")\n\n# Monitoring the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning Gemma 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:49:18.594516Z","iopub.execute_input":"2024-06-04T10:49:18.594902Z","iopub.status.idle":"2024-06-04T10:49:37.461543Z","shell.execute_reply.started":"2024-06-04T10:49:18.594870Z","shell.execute_reply":"2024-06-04T10:49:37.460224Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maymanboufarhi1\u001b[0m (\u001b[33mfstt1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240604_104920-8urex9xg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B/runs/8urex9xg' target=\"_blank\">restful-sponge-11</a></strong> to <a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B' target=\"_blank\">https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B/runs/8urex9xg' target=\"_blank\">https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B/runs/8urex9xg</a>"},"metadata":{}}]},{"cell_type":"code","source":"#Loading the dataset\ndataset = load_dataset(dataset_name, split=\"train[0:1000]\")\ndataset[\"text\"][100]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:49:37.464190Z","iopub.execute_input":"2024-06-04T10:49:37.464542Z","iopub.status.idle":"2024-06-04T10:49:39.398390Z","shell.execute_reply.started":"2024-06-04T10:49:37.464509Z","shell.execute_reply":"2024-06-04T10:49:39.397337Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/166k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8026f77433d041eb908c80f330656abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/276 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b6d56a221643dc88c39951d1dbb887"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"<|system|> FSTT c'est la Faculté des Sciences et Techniques de Tanger <</s>> <|user|>  Donne l'objectif de Biotechnologies (Options : animale et végétale) </s> <|assistant|> L’objectif de la licence Biotechnologie consiste à former des étudiants dans les différents champs disciplinaires de la biologie moderne et de permettre aux étudiants d’acquérir de solides connaissances scientifiques pour une réelle compréhension du Vivant. L’enseignement est conçu pour allier concepts fondamentaux et apprentissage des outils méthodologiques. Il est également conçu pour étendre les compétences au-delà du domaine de la biologie, en particulier vers le monde économique. Ce parcours de licence vise donc à :  Donner des bases fondamentales et des techniques dans le domaine de la biologie moléculaire, de la génétique, de l’immunologie, du génie microbiologique, de l’enzymologie, de la physiologie animale ou végétale, de la biotechnologie animale ou végétale, de l’Ecologie appliquée à la biologie et de la lutte biologique. Sensibiliser les étudiants aux évolutions technologiques des disciplines concernées. Permettre aux étudiants d'accéder à un Master. Intégrer la réflexion scientifique aux problèmes de société. </s>\""},"metadata":{}}]},{"cell_type":"code","source":"# Load base model(Gemma 2B-it)\nbnbConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnbConfig,\n        device_map=\"auto\"\n)\n\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:51:03.418892Z","iopub.execute_input":"2024-06-04T10:51:03.419261Z","iopub.status.idle":"2024-06-04T10:51:25.881543Z","shell.execute_reply.started":"2024-06-04T10:51:03.419235Z","shell.execute_reply":"2024-06-04T10:51:25.880488Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72aed3fac6df437ab5726fa269ebf018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4865aa01508049a0b31cf3737ed76c0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f934101d90954f75b3a96fe913a30655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d926439d8444e389444826badce910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a477501072054ae68a9d5c66986cf448"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f6f36f996e4130907d35b747e3a441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cdee30437054a5cbfe99f43d6379bde"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:51:47.009995Z","iopub.execute_input":"2024-06-04T10:51:47.010358Z","iopub.status.idle":"2024-06-04T10:51:48.785146Z","shell.execute_reply.started":"2024-06-04T10:51:47.010329Z","shell.execute_reply":"2024-06-04T10:51:48.784069Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de3bf7d02af47fdb00da8220cf73fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038531c14c794db8a99510144b7f4418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7d644ce2364be1b5327a29100d31f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f9669453c244bd8950fdca9630ead1"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:51:50.707240Z","iopub.execute_input":"2024-06-04T10:51:50.708016Z","iopub.status.idle":"2024-06-04T10:51:51.883465Z","shell.execute_reply.started":"2024-06-04T10:51:50.707981Z","shell.execute_reply":"2024-06-04T10:51:51.882400Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./gemma-2b-it-v2-fstt\",\n    num_train_epochs=18,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:18:53.828716Z","iopub.execute_input":"2024-06-04T11:18:53.829684Z","iopub.status.idle":"2024-06-04T11:18:53.859608Z","shell.execute_reply.started":"2024-06-04T11:18:53.829653Z","shell.execute_reply":"2024-06-04T11:18:53.858515Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:18:55.175863Z","iopub.execute_input":"2024-06-04T11:18:55.176238Z","iopub.status.idle":"2024-06-04T11:18:55.604125Z","shell.execute_reply.started":"2024-06-04T11:18:55.176209Z","shell.execute_reply":"2024-06-04T11:18:55.602904Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:18:58.710385Z","iopub.execute_input":"2024-06-04T11:18:58.710763Z","iopub.status.idle":"2024-06-04T12:04:05.181484Z","shell.execute_reply.started":"2024-06-04T11:18:58.710731Z","shell.execute_reply":"2024-06-04T12:04:05.180230Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2484' max='2484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2484/2484 45:03, Epoch 18/18]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.143500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.179200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.177600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.153700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.129600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.116900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.111700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.090800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.089700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.076300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.074600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.074200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.071100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.071400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.067000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.066800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.063700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.064200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.062600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.059400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2484, training_loss=0.09323269330361039, metrics={'train_runtime': 2705.9986, 'train_samples_per_second': 1.836, 'train_steps_per_second': 0.918, 'total_flos': 8941399784546304.0, 'train_loss': 0.09323269330361039, 'epoch': 18.0})"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:04:11.706075Z","iopub.execute_input":"2024-06-04T12:04:11.706443Z","iopub.status.idle":"2024-06-04T12:04:15.033558Z","shell.execute_reply.started":"2024-06-04T12:04:11.706413Z","shell.execute_reply":"2024-06-04T12:04:15.032754Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▅▆▆▇▇▅▄▆▄▃▃▃▃▃▅▄▆▄▃▂▂▃▃▂▂▂▂▂▁▁▁▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▄▄▃▃▂▂▁██▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>8941399784546304.0</td></tr><tr><td>train/epoch</td><td>18.0</td></tr><tr><td>train/global_step</td><td>2484</td></tr><tr><td>train/grad_norm</td><td>0.27</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0594</td></tr><tr><td>train_loss</td><td>0.09323</td></tr><tr><td>train_runtime</td><td>2705.9986</td></tr><tr><td>train_samples_per_second</td><td>1.836</td></tr><tr><td>train_steps_per_second</td><td>0.918</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">restful-sponge-11</strong> at: <a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B/runs/8urex9xg' target=\"_blank\">https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B/runs/8urex9xg</a><br/> View project at: <a href='https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B' target=\"_blank\">https://wandb.ai/fstt1/Fine%20tuning%20Gemma%207B</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240604_104920-8urex9xg/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"add your Hugging face acces token ( write ) :","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token $'write key'","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:04:18.025965Z","iopub.execute_input":"2024-06-04T12:04:18.026329Z","iopub.status.idle":"2024-06-04T12:04:19.617926Z","shell.execute_reply.started":"2024-06-04T12:04:18.026301Z","shell.execute_reply":"2024-06-04T12:04:19.616638Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the adaptor\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:04:22.092984Z","iopub.execute_input":"2024-06-04T12:04:22.093958Z","iopub.status.idle":"2024-06-04T12:04:33.417367Z","shell.execute_reply.started":"2024-06-04T12:04:22.093918Z","shell.execute_reply":"2024-06-04T12:04:33.416361Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/314M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3757782615c74d0b880c3f8955eb4125"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/aymanboufarhi/chat-bot-fstt/commit/0aa96fa8b6dc5c1140da8d0cdc9c244c7c6eaca7', commit_message='Upload model', commit_description='', oid='0aa96fa8b6dc5c1140da8d0cdc9c244c7c6eaca7', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"prompt = '''<|system|>FSTT c'est la Faculté des Sciences et Techniques de Tanger \n<|user|> Donne le Coordinnateur de Génie Informatique \n<|assistant|>'''\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}